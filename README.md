# GPU Hashingâ€¯Benchmark

*Comparative performance analysis of MD5, SHAâ€‘1 & SHAâ€‘256 on CPU **vs** CUDA GPU*

---

> **TL;DRÂ â€”** A midâ€‘range **NVIDIAÂ GTXâ€¯1650** running three overlapping CUDA streams hashes **18â€¯million strings in under one second**, roughly **10â€¯Ã— faster** than an 8â€‘core Ryzenâ€¯7â€¯3700X.

![CUDAÂ 11Â +](https://img.shields.io/badge/CUDA-11%2B-green)Â ![License:Â MIT](https://img.shields.io/badge/License-MIT-blue)

---

## Overview

This repository offersÂ â€”

* **Reference C implementations** of MD5, SHAâ€‘1 and SHAâ€‘256 for verification and CPU benchmarking.
* **Optimised CUDA kernels** that execute the same hashes massively in parallel.
* **Utility helpers** for batched I/O, timing, and result checking.
* **A Makeâ€‘based build system** targeting both CPUâ€‘only and GPUâ€‘accelerated binaries.
* **A full report (PDF)** capturing methodology, tuning sweeps, roofline plots and raw data.

The guiding question is **â€œHow much throughput can a commodity GPU deliver for bulk hashing, and which optimisation levers matter most?â€**

---

## Features

| Feature                          | Description                                                                                     |
| -------------------------------- | ----------------------------------------------------------------------------------------------- |
| **BatchÂ processing**             | Streams arbitrarily large datasets through a small, reusable buffer â€‘ runs even on 4â€¯GB cards.  |
| **Tripleâ€‘buffer CUDAÂ streams**   | Overlaps Hostâ†’Device copies, kernel execution and Deviceâ†’Host copies for nearÂ­â€‘max utilisation. |
| **Oneâ€‘line configuration**       | Flip batching, streams, block size & more in `include/config.h`Â â€” no source edits needed.       |
| **Crossâ€‘checked correctness**    | CPU and GPU digests are compared at runtime; any mismatch aborts execution.                     |
| **Lightweight datasets viaÂ LFS** | Large test files reside in GitÂ LFS so the repository clone stays trim.                          |

---

## ProjectÂ Structure

```
.
â”œâ”€â”€ Makefile              # Targets: cpu, gpu, gpu_streams, clean â€¦
â”œâ”€â”€ include/              # Public headers & switches
â”‚   â””â”€â”€ config.h          # âœï¸ Â Dataset path, batch size, stream countâ€¦
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cpu source files  # md5.c, sha1.c, sha256.c  (reference)
â”‚   â”œâ”€â”€ gpu source files  # md5_kernel.cu, â€¦         (CUDA kernels)
â”‚   â””â”€â”€ utilities.c       # I/O helpers, timers, pretty printers
â”œâ”€â”€ dataset/              # Large CSV test files (GitÂ LFS)
â””â”€â”€ report/
    â””â”€â”€ report.pdf        # Full writeâ€‘up with graphs & analysis
```

---

## Prerequisites

* **CUDA ToolkitÂ 11.0â€¯+**
* **GPUÂ Compute CapabilityÂ 7.5â€¯+** (GTXâ€¯1650 or newer)
* **GCCÂ 9â€¯+/Â ClangÂ 11â€¯+**Â (tested with GCCÂ 13)
* **GitÂ LFS** for dataset checkout
* *Optional:* **CMake**Â if you prefer over Make

```bash
# Enable LFS once per machine
git lfs install
```

---

# How to clone the repository

Since dataset files are big, they are stored in the repository using `lfs`, so if the repository is cloned using ssh ensure to own `git lfs` with the command `git lfs --version`. If `git lfs` is not present then it can be installed using `git lfs install`. If `git lfs` cannot be used / downloaded, then it is required to download manually the datasets from the repository as raw files. \
Please note that in case of repository downloading as `zip file`, the dataset still need to be download as raw files from the dataset folder.

# Quick start

1. Extract a file from the dataset folder and put the name of the file in the `include/config.h` file under the `#define FILENAME_PATH` macro.
2. Edit the Makefile according to the features of the GPU used to run the code (more specifically edit the root directory of CUDA binaries and the SM_ARCH flag).
3. Edit the `include/config.h` file, and set the configuration to run the code, i.e. decide whether to use STREAM execution or not, whether to use the batch processing (if the dataset is too large to fit either in the CPU or the GPU ram memory) and so on.
4. Now type `make` in the terminal and then run the binary generated using the command `./exe/test`.


## Configuration (`include/config.h`)

| Macro             | Role                              | Default                   |
| ----------------- | --------------------------------- | ------------------------- |
| `FILENAME_PATH`   | Input file path                   | `"dataset/18M_lines.txt"` |
| `USE_STREAMS`     | Enable CUDA stream overlap        | `1`                       |
| `STREAM_COUNT`    | Concurrent streams                | `3`                       |
| `USE_BATCHING`    | Recycle fixedâ€‘size buffers        | `1`                       |
| `BATCH_NUM_LINES` | Lines per batch (if batching = 1) | `131072`                  |
| `BLOCK_SIZE`      | CUDA threads per block            | `256`                     |

Reâ€‘run `make` after editing.

---

## Benchmarks

*Hardware:* **RyzenÂ 7â€¯3700Xâ€¯@â€¯4.4â€¯GHz** vs **GTXâ€¯1650Â (896Â cores,Â 4â€¯GB,Â CUDAÂ 12)** using default config.

|  18â€¯MÂ lines | CPU (ms) | GPU (ms) | GPUÂ +Â Streams (ms) |   Speedâ€‘up |
| ----------: | -------: | -------: | -----------------: | ---------: |
|     **MD5** |    7â€¯804 |    1â€¯107 |            **804** |  **9.7â€¯Ã—** |
|   **SHAâ€‘1** |    9â€¯467 |    1â€¯203 |            **884** | **10.7â€¯Ã—** |
| **SHAâ€‘256** |   10â€¯234 |    1â€¯309 |            **899** | **11.4â€¯Ã—** |

ğŸ›ˆ *For <â€¯1â€¯M lines the CPU wins due to GPU launch overhead; beyond that, the GPU dominates.*

Detailed graphs and profiler screenshots live in [`report/report.pdf`](report/gpu_programming_report_group2.pdf).

---

## ProfilingÂ Insights

* Kernels achieve **63â€‘70â€¯%** of peak integer ALU throughput while using **<â€¯50â€¯%** of memory bandwidth â†’ *computeâ€‘bound*.
* Tripleâ€‘buffer streaming offers a **15â€‘20â€¯%** free boost by hiding PCIe latency.
* A block size of 256 threads strikes the best occupancy/pressure balance.

---

## Limitations

1. Accelerates **large batches only**; singleâ€‘hash latency is still CPUâ€‘faster.
2. MD5 & SHAâ€‘1 are **insecure**; keep them for benchmarks, use SHAâ€‘256/SHAâ€‘3 in production.
3. CPU SHAâ€‘extensions were **disabled** in the baseline; enabling them narrows the gap.


## License

Released under the **MIT License** â€” see [`LICENSE`](LICENSE).

## Authors
- [Tommaso Gualtierotti](https://github.com/tommasogualtierotti)
- [Simone Mulazzi](https://github.com/mulaz1)

**Happy hashing!** ğŸš€
